{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25377f83-4eb1-4a10-94f8-1db941f5c707",
   "metadata": {},
   "source": [
    "## Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe3b6916-31dd-4f43-9df1-f181c303f8d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/iris/Documents/Uni/AIR/WineRecommender\n"
     ]
    }
   ],
   "source": [
    "# make sure we are in WineRecommender so the paths work\n",
    "import os\n",
    "if 'evaluation' in os.getcwd():\n",
    "    os.chdir(\"..\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77162d4-8d0c-4c72-aa55-d11c28087c69",
   "metadata": {},
   "source": [
    "## MRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67d83b8c-6c09-4e17-9750-a55bb9f23adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR Model: 0.5\n",
      "MRR Baseline: 0.6066666666666667\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Higher MRR indicates that relevant documents appear earlier in the rankings.\n",
    "\n",
    "def calculate_MRR(model):\n",
    "    ratings = scores[\"queries\"]\n",
    "    number_of_queries = len(ratings)\n",
    "\n",
    "    total = 0\n",
    "    for rating in ratings:\n",
    "        rank_first_relevant = 0 # relevant: score = 3\n",
    "        for wine in rating[model]: # loop over the 10 wines returned\n",
    "            if wine[\"score\"] == \"2\" or wine[\"score\"] == \"3\":\n",
    "                rank_first_relevant = wine[\"rank\"]\n",
    "                break\n",
    "\n",
    "        if rank_first_relevant != 0:\n",
    "            total += 1/rank_first_relevant\n",
    "    \n",
    "    return total/number_of_queries\n",
    "\n",
    "\n",
    "\n",
    "with open(\"evaluation/scores.json\", \"r\") as file:\n",
    "    scores = json.load(file)\n",
    "\n",
    "\n",
    "print(f\"MRR Model: {calculate_MRR('model')}\")\n",
    "print(f\"MRR Baseline: {calculate_MRR('baseline')}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a41e44-5b91-45b9-a3a3-bf3bc4b15c05",
   "metadata": {},
   "source": [
    "## MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0bae86a8-6872-4bc2-b731-2523f4436bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP Model: 0.4510959939531368\n",
      "MAP Baseline: 0.5171111111111111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iris/micromamba/envs/jupyter/lib/python3.11/site-packages/sklearn/metrics/_ranking.py:1131: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def calculate_MAP(model):\n",
    "    ratings = scores[\"queries\"]\n",
    "    number_of_queries = len(ratings)\n",
    "\n",
    "    total = 0\n",
    "    for rating in ratings:\n",
    "        true_values = []\n",
    "        for wine in rating[model]:\n",
    "            if wine[\"score\"] == \"2\" or wine[\"score\"] == \"3\":\n",
    "                true_values.append(1) # relevant\n",
    "            else:\n",
    "                true_values.append(0)\n",
    "\n",
    "        # print(true_values)\n",
    "        total += average_precision_score(true_values, [1, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1])\n",
    "    \n",
    "    return total/number_of_queries\n",
    "\n",
    "    \n",
    "# our system ranked the first result best, the second result second best, ... -> [1, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1]\n",
    "\n",
    "print(f\"MAP Model: {calculate_MAP('model')}\")\n",
    "print(f\"MAP Baseline: {calculate_MAP('baseline')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9e8dcf-9a41-47d9-8655-6dcf88271492",
   "metadata": {},
   "source": [
    "## nDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d51a9217-e5ee-4b00-a088-1b70d8030a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nDCG Model: 0.8730615632467046\n",
      "nDCG Baseline: 0.8275158597358917\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import ndcg_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def calculate_NDCG(model):\n",
    "    ratings = scores[\"queries\"]\n",
    "    number_of_queries = len(ratings)\n",
    "\n",
    "    true_values = []\n",
    "    rank_values = []\n",
    "    for rating in ratings:\n",
    "        true_values_per_query = []\n",
    "        for wine in rating[model]:\n",
    "            if wine[\"score\"] == None:\n",
    "                continue\n",
    "            true_values_per_query.append(int(wine[\"score\"]))\n",
    "        if len(true_values_per_query) != 10:\n",
    "            print(f\"None value detected at {rating['query']}\")\n",
    "            continue\n",
    "        true_values.append(true_values_per_query)\n",
    "        rank_values.append([1, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1])\n",
    "    \n",
    "    return ndcg_score(true_values, rank_values)\n",
    "    \n",
    "\n",
    "print(f\"nDCG Model: {calculate_NDCG('model')}\")\n",
    "print(f\"nDCG Baseline: {calculate_NDCG('baseline')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67f083c-b8cc-43a9-9685-79605f01ccab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
